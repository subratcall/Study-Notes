**1.[Dropout is a special case of the stochastic delta rule- faster and more accurate deep](https://arxiv.org/pdf/1808.03578v1.pdf)** was submmited to NIPS and on arXiv. Stochastic Delta Rule (SDR) is a weight update mechanism that assigns to each weight a standard deviation that changes as a function of the gradients every training iteration. At the beginning of each training iteration, the weights are re-initialized using a normal distribution bound by their standard deviations. Over the course of the training iterations and epochs, the standard deviations converge towards zero as the network becomes more sure of what the values of each of the weights should be. For a more detailed description of the method and its properties, have a look at the paper. If you want to look the code, see **[the github respository](https://github.com/noahfl/densenet-sdr)**.

**2.[Densely Connected Convolutional Networks]( https://arxiv.org/abs/1608.06993)** got Best Paper Award on CVPR 2017. DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each dense block). For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. This connectivity pattern yields state-of-the-art accuracies on CIFAR10/100 (with or without data augmentation) and SVHN. On the large scale ILSVRC 2012 (ImageNet) dataset, DenseNet achieves a similar accuracy as ResNet, but using less than half the amount of parameters and roughly half the number of FLOPs. If you want to look the code, see **[the github respository](https://github.com/liuzhuang13/DenseNet)**.
