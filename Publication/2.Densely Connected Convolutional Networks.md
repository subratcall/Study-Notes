The arXiv link is https://arxiv.org/abs/1608.06993.  

The github repository is https://github.com/liuzhuang13/DenseNet

Notes:

<div align=center><img src="https://github.com/Quan-Sun/Study-Notes/blob/master/Publication/images/DenseNet1.png"/></div>

- Instead of drawing representational power from ex- tremely deep or wide architectures, DenseNets exploit the potential of the 
network through feature reuse, yielding con- densed models that are easy to train and highly parameter- efficient. 

- Concatenating feature-maps learned by different layers increases variation in the input of subsequent layers and improves efficiency. 

 <div align=center><img src="https://github.com/Quan-Sun/Study-Notes/blob/master/Publication/images/DenseNet2.png"/></div>

- Traditional convolutional feed-forward net- works connect the output of the lth layer as input to the (l + 1)th layer, which gives rise to the following layer transition: xl = Hl (xl−1 ).
ResNets add a skip-connection that bypasses the non-linear transforma- tions with an identity function: xl = Hl(xl−1) + xl−1. An advantage of ResNets is that the gradient can flow di- rectly 
through the identity function from later layers to the earlier layers. However, the identity function and the output of Hl are combined by summation, which may impede the information flow in the network.

- The DenseNets introduces direct connections from any layer to all subsequent layers. The lth layer receives the feature-maps of all preceding layers, x0, . . . , xl−1, as input:
xl = Hl([x0,x1,...,xl−1]), where [x0, x1, . . . , xl−1] refers to the concatenation of the feature-maps produced in layers 0, . . . , l − 1.

- Define Hl(·) as a composite function of three consecutive operations: batch normalization (BN), followed by a rectified lin- ear unit (ReLU) and a 3 × 3 convolution (Conv).

- We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normal- ization layer and an 1×1 convolutional layer followed by a 2×2 average pooling layer.

- If each function Hl produces k feature- maps, it follows that the lth layer has k0 + k × (l − 1) input feature-maps, where k0 is the number of channels in the in- put layer. We refer to the hyper- parameter k as the growth rate of the network.

- We refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv(1× 1)-BN-ReLU-Conv(3×3) version of Hl, as DenseNet-B. In our experiments, we let each 1×1 convolution produce 4k feature-maps.

- To further improve model compactness, we can reduce the number of feature-maps at transition layers. If a dense block contains m feature-maps, we let the following transition layer generate ⌊θm⌋ output feature- maps, where 0 < θ ≤ 1 is referred to as the compression factor.
We refer the DenseNet with θ < 1 as DenseNet-C, and we set θ = 0.5 in our experiment. When both the bottleneck and transition layers with θ < 1 are used, we refer to our model as DenseNet-BC.

<div align=center><img src="https://github.com/Quan-Sun/Study-Notes/blob/master/Publication/images/DenseNet3.png"/></div>

<div align=center><img src="https://github.com/Quan-Sun/Study-Notes/blob/master/Publication/images/DenseNet4.png"/></div>

