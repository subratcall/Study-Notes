
**1.[Regression Tree](https://github.com/Quan-Sun/Study-Notes/blob/master/Algoritm/1.RegressionTree.ipynb)** - A notebook introduces algorithm of `regression tree` which is a particular kind of nonlinear predictive model. You can understand clearly by reading [this CMU lecture](http://www.stat.cmu.edu/~cshalizi/350-2006/lecture-10.pdf).
<div align=center><img src="https://github.com/Quan-Sun/Study-Notes/blob/master/Algoritm/images/regressionTree.jpeg" width="500" height="300"/></div>

**2.[C4.5](https://github.com/Quan-Sun/Study-Notes/blob/master/Algoritm/2.C4.5.ipynb)** - A notebook introduces algorithm of `C4.5` which generates classifiers expressed as decision trees. C4.5 adopts information gain ratio as criterion and PEP (Pessimistic Error Pruning, which uses error ratio as criterion) to prune trees. Comparing with ID3 the C4.5 can handle atrributes with continuous values. You can understand clearly by reading [Ross Quinlan web](http://www.rulequest.com/Personal/). If you want to know more about decision tree, I reconmmend [this repo](https://github.com/michaeldorner/DecisionTrees).
<div align=center><img src="https://github.com/Quan-Sun/Study-Notes/blob/master/Algoritm/images/C4.5_graph.png" width="500" height="300"/></div>
<div align=center>Iris as an example by C4.5</div>


**3.[K-Means](https://github.com/Quan-Sun/Study-Notes/blob/master/Algoritm/3.K-Means.ipynb)** - A notebook introduces algorithm of `K-Means` which is used to find groups that have not been explicitly labeled in the data. You can understand clearly by looking Siraj Raval Youtube [video](https://youtu.be/9991JlKnFmk).

**4.[SVM](https://github.com/Quan-Sun/Study-Notes/blob/master/Algoritm/3.K-Means.ipynb)** - A notebook introduces algorithm of `Support Vector Machine` which is a supervised machine learning algorithm using for both classification or regression challenges. Given two or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.
<div align=center><img src="https://github.com/Quan-Sun/Study-Notes/blob/master/Algoritm/images/svm.png" width="500" height="300"/></div>
You can understand more clear by reading:

 - [Support-Vector Networks](https://link.springer.com/article/10.1007%2FBF00994018) by Cortes and Vapnik 1995
 - [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine) by wikipedia
 - [Support Vector Machines for Machine Learning](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/) by Jason Brownlee

